# Paper-list-for-AI
From Ilya Sutskever Recommended Reading + Other Influential Papers

## üìö Ilya Sutskever's Recommended Reading List

> ‚ÄúIf you deeply understand all of these, you‚Äôll know 90% of what matters today.‚Äù  
> ‚Äî Ilya Sutskever, shared with John Carmack

1. **[Transformer Visualization]** [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
2. **[Thermodynamics & Complexity]** [The First Law of Complexodynamics](https://scottaaronson.blog/?p=762)
3. **[RNN Intuition]** [The Unreasonable Effectiveness of RNNs](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
4. **[LSTM Tutorial]** [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
5. **[RNN Regularization]** [Recurrent Neural Network Regularization (Zaremba et al., 2014)](https://arxiv.org/pdf/1409.2329.pdf)
6. **[MDL in Neural Nets]** [Keeping Neural Networks Simple by Minimizing the Description Length of the Weights (Hinton, van Camp, 1993)](https://www.cs.toronto.edu/~hinton/absps/colt93.pdf)
7. **[Pointer Networks]** [Pointer Networks (Vinyals et al., 2015)](https://arxiv.org/pdf/1506.03134.pdf)
8. **[CNN Landmark]** [ImageNet Classification with Deep CNNs (Krizhevsky et al., 2012)](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
9. **[Set-to-Sequence Modeling]** [Order Matters: Sequence to sequence for sets (Vinyals et al., 2015)](https://arxiv.org/pdf/1511.06391.pdf)
10. **[Model Parallelism]** [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism (Huang et al., 2018)](https://arxiv.org/pdf/1811.06965.pdf)
11. **[Residual Networks]** [Deep Residual Learning for Image Recognition (He et al., 2015)](https://arxiv.org/pdf/1512.03385.pdf)
12. **[Dilated Convolutions]** [Multi-Scale Context Aggregation by Dilated Convolutions (Yu & Koltun, 2015)](https://arxiv.org/pdf/1511.07122.pdf)
13. **[Message Passing for Molecules]** [Neural Message Passing for Quantum Chemistry (Gilmer et al., 2017)](https://arxiv.org/pdf/1704.01212.pdf)
14. **[Transformer Architecture]** [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/pdf/1706.03762.pdf)
15. **[Neural Alignment]** [Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau et al., 2014)](https://arxiv.org/pdf/1409.0473.pdf)
16. **[ResNet Optimization]** [Identity Mappings in Deep Residual Networks (He et al., 2016)](https://arxiv.org/pdf/1603.05027.pdf)
17. **[Relational Reasoning]** [A Simple Neural Network Module for Relational Reasoning (Santoro et al., 2017)](https://arxiv.org/pdf/1706.01427.pdf)
18. **[Variational Autoencoder]** [Variational Lossy Autoencoder (Chen et al., 2016)](https://arxiv.org/pdf/1611.02731.pdf)
19. **[Relational RNN]** [Relational Recurrent Neural Networks (Santoro et al., 2018)](https://arxiv.org/pdf/1806.01822.pdf)
20. **[Cellular Automata Complexity]** [Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton (Aaronson et al., 2014)](https://arxiv.org/pdf/1405.6903.pdf)
21. **[Memory-Augmented Models]** [Neural Turing Machines (Graves et al., 2014)](https://arxiv.org/pdf/1410.5401.pdf)
22. **[End-to-End Speech Recognition]** [Deep Speech 2: End-to-End Speech Recognition (Amodei et al., 2015)](https://arxiv.org/pdf/1512.02595.pdf)
23. **[Scaling Laws]** [Scaling Laws for Neural Language Models (Kaplan et al., 2020)](https://arxiv.org/pdf/2001.08361.pdf)
24. **[Information Theory]** [A Tutorial Introduction to the Minimum Description Length Principle (Gr√ºnwald, 2004)](https://arxiv.org/pdf/math/0406077.pdf)
25. **[Superintelligence Foundations]** [Machine Super Intelligence Dissertation (Shane Legg, 2008)](https://www.vetta.org/documents/Machine_Super_Intelligence.pdf)
26. **[Kolmogorov Complexity]** [Kolmogorov Complexity: Page 434 onward (Shen, Uspensky, Vereshchagin)](https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf)
27. **[CNN Course]** [CS231n: CNNs for Visual Recognition (Stanford)](https://cs231n.github.io/)

---

## üìö Other Influential Papers
ÂèÇËÄÉÁîüÂëΩÊû∂ÊûÑÂπ≥Âè∞ LifeArchitect.ai Êé®ËçêÁöÑÂ§ñÈÉ®ÈáçË¶ÅËÆ∫ÊñáÔºåË°•ÂÖÖ Ilya ÂéüÂßãÂàóË°®‰πãÂ§ñÁöÑÈáçË¶ÅËµÑÊ∫êÔºö

- **[GPT-4 Overview]** [GPT‚Äë4 Technical Report](https://arxiv.org/pdf/2303.08774.pdf) ‚Äï OpenAI, 2023
- **[System Safety]** [GPT‚Äë4 System Card](https://cdn.openai.com/papers/gpt-4-system-card.pdf) ‚Äï OpenAI, 2023
- **[PaLM 2 Overview]** [PaLM¬†2 Technical Report](https://storage.googleapis.com/deepmind-media/palm2techreport/palm2.pdf) ‚Äï Google, 2023
- **[Sparks of AGI]** [Sparks of Artificial General Intelligence: Early Experiments with GPT‚Äë4](https://arxiv.org/pdf/2303.12712.pdf) ‚Äï Bubeck et al., 2023
- **[GPT‚Äë4.5 Preview]** [GPT‚Äë4.5 ‚ÄúOrion‚Äù Overview](https://openai.com/index/hello-gpt-4o) ‚Äï OpenAI, Feb 2025
- **[GPT‚Äë4.1 Updates]** [GPT‚Äë4.1 Overview](https://openai.com/index/hello-gpt-4o) ‚Äï OpenAI, Apr 2025

- **[Claude 3 Model Card]** [Claude 3 Opus](https://www.anthropic.com/claude-3-model-card) ‚Äï Anthropic, 2024
- **[Gemini 1.0]** [Gemini Technical Report](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf) ‚Äï Google DeepMind, 2023
- **[PaLM 1.0]** [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf) ‚Äï Chowdhery et al., 2022
- **[Chinchilla Scaling Law]** [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf) ‚Äï Hoffman et al., 2022
- **[Generalist Agent]** [Gato: A Generalist Agent](https://arxiv.org/pdf/2205.06175.pdf) ‚Äï Reed et al., 2022
- **[LLM-Robot Interface]** [Do As I Can, Not As I Say](https://arxiv.org/pdf/2204.01691.pdf) ‚Äï Ahn et al., 2022
- **[Transformer Algorithms]** [Formal Algorithms for Transformers](https://arxiv.org/pdf/2207.09238.pdf) ‚Äï Phuong & Hutter, DeepMind, 2022

- **[GPT-3 Foundation]** [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf) ‚Äï Brown et al., 2020
- **[GPT-2 Scaling]** [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ‚Äï Radford et al., 2019
- **[GPT-1 Proposal]** [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) ‚Äï Radford et al., 2018

- **[RoBERTa]** [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf) ‚Äï Liu et al., 2019
- **[BERT]** [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/pdf/1810.04805.pdf) ‚Äï Devlin et al., 2019
- **[ULMFiT]** [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146.pdf) ‚Äï Howard & Ruder, 2018

- **[Transformer Origin]** [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) ‚Äï Vaswani et al., 2017

- **[Perceptron Controversy]** [A Sociological Study of the Official History of the Perceptrons Controversy](https://www.dsi.unive.it/~ml/Olazaran1996.pdf) ‚Äï Olazaran, 1996
- **[Turing Test]** [Computing Machinery and Intelligence](https://www.csee.umbc.edu/courses/471/papers/turing.pdf) ‚Äï Alan Turing, 1950

- **[Early AI Vision]** [Intelligent Machinery (Turing, 1948)](https://www.aisb.org.uk/publications/aisbquarterly/aisbq112.pdf) ‚Äî Rediscovered & typed by ‚ÄòGabriel‚Äô  

---

## From my Zotero
- **[ViT]** [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

## üîó Credits

- Based on public share by Ilya Sutskever to John Carmack.
- Curated by the community with ‚ù§Ô∏è.
